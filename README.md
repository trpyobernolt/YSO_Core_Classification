# midterm_Fall2022STRUCTUREmain.py: runs code for 3 selected modelsmidterm_statistics: contains methods for calulating statistics of the datamidterm_model: most important module. This module contains the Model class.     This class contains methods for trimming, scaling, and running the model.    This model can be used for any machine learning model that was given and     was used dynamically to test 9 potential methods.optimization.py: This module is not used in the main script. It contains methods    that were used for optimization purposes.            INTRODUCTIONThis project is meant as an introduction to classification algorithms. Thebasic outline of this project is to load the data into a pandas dataframe,engineer the data so that is readable by machine learning algorithms, splittingthe data into training and test sets, scaling the data for better precision,and finally actually training the data and preciting values.Personally, the most difficult part of this process was engineering the data.My data included both empty spaces and ----- to indicate missing data. This required the usage of regular expressions to find and replace data. I personallyused a "most frequent" replacement method for my X values, as some of the datawas extremely large and some was extremely small, likely signlaing some aspectof what we are observing. Using this method as opposed to a mean replacementwould help to conserve information contained in the data that could be relevant.I did the same for missing y labels. SCALINGI tried all of the different scalers to see what would work best with my data.There were many combinations, but I found that MaxAbsScaler worked best and withoutany other scalers. ALGORITHMSThe three algorithms that I chose were: Random Forest Classification, DecisionTree, and K Nearest Neighbors.The decision tree algorithm is one of the most intuitive to understand to me.The idea stems from a simple if statement. If:x, then :y. The algorithm itselfis a result of entropy - each step down the decision tree gains information about the data and thefore gives a closer classification. The training of thetree is then defined by recursion. If we can find the information gained by making a decision based on a set of parameters at a node, we can recursivelyfind information gained past that node. Using this base concept, the decisiontree grows to its leafs. When a new argument is passed into this algorithm,it simply follows the decision tree down following its rules until a conclusionis drawn. The classifier can take a max depth - this defines how many decisionsare made until the algorithm makes a decision.I chose Random Forest Classification because it gave me the best results aftertrying all of the other methods. This algorithm is very similar to the decisiontree algorithm, but differs in a key way. Instead of just creating trees,the Random Forest model creats a forest of decision trees that operate in fundamentally the same way. The mathematics behind the algorithm function inroughly the same way, but the Random Forest Classification corrects for the overfitting problem that decision trees usually face. The algorithm usuallytakes two parameters: the number of trees to be included in the forest,the maximum depth of the trees within the forest, and the number of featuresto look at at each split. When a new element is passed in for classification, the element is passed through each of the trees, which then vote and a finaldecision is made.The K-nearest-neighbors algorithm is fairly easy to understand and was discussedin class. The algorithm isn't really trained, instead it just stores the datasetfor further use. When a new element is passed in, it compares the data to eachelement in the training set and determines the relative closeness to it (thiscan be done through cosine similariaty). The algorithm has an arugment of how manyneighbors to compare the element to. The algorithm takes the classification of the user-defined nearest neighbors, finds the most common classification, anduses this as the label for the new data.ResultsRunning the main script, I found the following results:Random Forest Classifier:    Accuracy = 0.7916666666666666     Precision: 0.81    Recall:0.79    F1-score: 0.77    Decision Tree:    Accuracy: 0.66666666666666666    Precision: 0.68    Recall: 0.53    f1-score: 0.67    K-nearest Neighbors:    Accuracy: 0.5416666666666666    Precision: 0.54    Recall: 0.54    f1-score: 0.52    DiscussionAlthough I had hoped for better results, even with fine tuning this is the bestI could produce. The accuracy score describes the percentages of correct classificaitons (total correct / total predictions). Precision describes how gooda model is at classifying a specific category (TruePositives / (TruePositives + FalsePositives). Recall describes how good a model is at finding a node with a specific category (TruePositives / (TruePositives + FalseNegatives)). The f1 score is a way of combining the precision and recall of a model for another usefulmetric (2 / ( (1/recall) * (1/precision) )In all metrics, the random forest classifier performed the best. It was able to predict the correct classification 79.2% of the time (Nearly the threshold 80%if you're feeling nice, Quang). It had a precision of 81%, meaning that it was able to classify all points in a given class 81% of the time. It had a recall of 79% and an f1-score of 77%.Although these scores are fairly low, they are realistic predictors with realisticdata that contains noise.Summary Statistics (output from summary_stats() in statistics module)---------------------Signi070 Has: 	Standard Deviation of 22.377833153964364	Mean of 12.28	Range of 113.0	Minimum of 0.0	Maximum of 113.0Sp070 Has: 	Standard Deviation of 40.97031807160995	Mean of 24.967333333333332	Range of 156.0	Minimum of 0.0	Maximum of 156.0e_Sp070 Has: 	Standard Deviation of 21.740794283634933	Mean of 13.9885	Range of 108.0	Minimum of 0.0	Maximum of 108.0Sp/Sbg070 Has: 	Standard Deviation of 25.925570107842695	Mean of 15.3775	Range of 98.0	Minimum of 0.0	Maximum of 98.0Sconv070 Has: 	Standard Deviation of 34.870559655197205	Mean of 20.505999999999997	Range of 135.0	Minimum of 0.0	Maximum of 135.0Stot070 Has: 	Standard Deviation of 22.204136038635284	Mean of 16.648333333333333	Range of 100.0	Minimum of 0.0	Maximum of 100.0e_Stot070 Has: 	Standard Deviation of 28.110183462680485	Mean of 18.08583333333333	Range of 148.0	Minimum of 0.0	Maximum of 148.0FWHMa070 Has: 	Standard Deviation of 25.411115841409945	Mean of 20.099833333333333	Range of 85.0	Minimum of 0.0	Maximum of 85.0FWHMb070 Has: 	Standard Deviation of 34.54445386261978	Mean of 20.997999999999998	Range of 128.0	Minimum of 0.0	Maximum of 128.0PA070 Has: 	Standard Deviation of 32.49852622769565	Mean of 21.35883333333333	Range of 115.0	Minimum of 0.0	Maximum of 115.0Signi160 Has: 	Standard Deviation of 22.162101484792053	Mean of 15.923333333333336	Range of 106.0	Minimum of 0.0	Maximum of 106.0Sp160 Has: 	Standard Deviation of 24.094464195928676	Mean of 15.878666666666666	Range of 84.0	Minimum of 0.0	Maximum of 84.0e_Sp160 Has: 	Standard Deviation of 30.617998596958184	Mean of 20.5945	Range of 108.0	Minimum of 0.0	Maximum of 108.0Sp/Sbg160 Has: 	Standard Deviation of 38.29081193061275	Mean of 23.130166666666664	Range of 161.0	Minimum of 0.0	Maximum of 161.0Sconv160 Has: 	Standard Deviation of 36.96057473511471	Mean of 29.6805	Range of 125.0	Minimum of 0.0	Maximum of 125.0Stot160 Has: 	Standard Deviation of 27.840009865898637	Mean of 17.718	Range of 150.0	Minimum of 0.0	Maximum of 150.0e_Stot160 Has: 	Standard Deviation of 24.63935648679432	Mean of 17.330499999999997	Range of 113.0	Minimum of 0.0	Maximum of 113.0FWHMa160 Has: 	Standard Deviation of 38.19579552887115	Mean of 21.0965	Range of 174.0	Minimum of 0.0	Maximum of 174.0FWHMb160 Has: 	Standard Deviation of 40.54197313176391	Mean of 22.2675	Range of 171.0	Minimum of 0.0	Maximum of 171.0PA160 Has: 	Standard Deviation of 36.88233514238972	Mean of 26.716666666666665	Range of 149.0	Minimum of 0.0	Maximum of 149.0Signi250 Has: 	Standard Deviation of 161.7624894917099	Mean of 80.81366666666666	Range of 822.0	Minimum of 0.0	Maximum of 822.0Sp250 Has: 	Standard Deviation of 13.388707243004788	Mean of 11.575166666666668	Range of 50.0	Minimum of 0.0	Maximum of 50.0e_Sp250 Has: 	Standard Deviation of 18.979880250032018	Mean of 10.494166666666667	Range of 127.0	Minimum of 0.0	Maximum of 127.0Sp/Sbg250 Has: 	Standard Deviation of 34.36683999107602	Mean of 25.189166666666665	Range of 141.0	Minimum of 0.0	Maximum of 141.0Sconv250 Has: 	Standard Deviation of 27.52823371054203	Mean of 16.105666666666664	Range of 162.0	Minimum of 0.0	Maximum of 162.0Stot250 Has: 	Standard Deviation of 24.127880145291577	Mean of 16.447166666666668	Range of 134.0	Minimum of 0.0	Maximum of 134.0e_Stot250 Has: 	Standard Deviation of 35.31855409302909	Mean of 24.509666666666664	Range of 125.0	Minimum of 0.0	Maximum of 125.0FWHMa250 Has: 	Standard Deviation of 34.61331613178502	Mean of 22.487833333333334	Range of 157.0	Minimum of 0.0	Maximum of 157.0FWHMb250 Has: 	Standard Deviation of 30.828542952198628	Mean of 19.418333333333333	Range of 137.0	Minimum of 0.0	Maximum of 137.0PA250 Has: 	Standard Deviation of 27.199107977101015	Mean of 18.9045	Range of 93.0	Minimum of 0.0	Maximum of 93.0Signi350 Has: 	Standard Deviation of 46.06282311842044	Mean of 33.41883333333333	Range of 171.0	Minimum of 0.0	Maximum of 171.0Sp350 Has: 	Standard Deviation of 45.423919463391385	Mean of 26.0665	Range of 170.0	Minimum of 0.0	Maximum of 170.0e_Sp350 Has: 	Standard Deviation of 38.453366036740604	Mean of 21.152666666666665	Range of 171.0	Minimum of 0.0	Maximum of 171.0Sp/Sbg350 Has: 	Standard Deviation of 368.60688612045317	Mean of 142.14249999999998	Range of 1823.0	Minimum of 0.0	Maximum of 1823.0Sconv350 Has: 	Standard Deviation of 534.0113347157427	Mean of 183.68133333333336	Range of 3000.0	Minimum of 0.0	Maximum of 3000.0Stot350 Has: 	Standard Deviation of 42.41176121568953	Mean of 23.8365	Range of 172.0	Minimum of 0.0	Maximum of 172.0e_Stot350 Has: 	Standard Deviation of 592.3712643982234	Mean of 257.8105	Range of 2708.0	Minimum of 0.0	Maximum of 2708.0FWHMa350 Has: 	Standard Deviation of 44.95823868232236	Mean of 27.3275	Range of 155.0	Minimum of 0.0	Maximum of 155.0FWHMb350 Has: 	Standard Deviation of 5.888878406699263e+16	Mean of 7666666666666721.0	Range of 4.6e+17	Minimum of 0.0	Maximum of 4.6e+17PA350 Has: 	Standard Deviation of 25.366893759386468	Mean of 13.599	Range of 173.0	Minimum of 0.0	Maximum of 173.0Signi500 Has: 	Standard Deviation of 106.59143828657157	Mean of 56.808	Range of 440.0	Minimum of 0.0	Maximum of 440.0Sp500 Has: 	Standard Deviation of 101.64599272789306	Mean of 56.704166666666666	Range of 482.0	Minimum of 0.0	Maximum of 482.0e_Sp500 Has: 	Standard Deviation of 600.820903064646	Mean of 232.8235	Range of 3285.0	Minimum of 0.0	Maximum of 3285.0Sp/Sbg500 Has: 	Standard Deviation of 35.03931339541661	Mean of 23.519666666666662	Range of 161.0	Minimum of 0.0	Maximum of 161.0Stot500 Has: 	Standard Deviation of 869.0101883332394	Mean of 320.94716666666665	Range of 4048.0	Minimum of 0.0	Maximum of 4048.0e_Stot500 Has: 	Standard Deviation of 36.971040397631945	Mean of 25.824499999999997	Range of 175.0	Minimum of 0.0	Maximum of 175.0FWHMa500 Has: 	Standard Deviation of 23.099640543138808	Mean of 16.339666666666666	Range of 70.0	Minimum of 0.0	Maximum of 70.0FWHMb500 Has: 	Standard Deviation of 1480.863107296355	Mean of 600.4704999999999	Range of 6868.33	Minimum of 0.67	Maximum of 6869.0PA500 Has: 	Standard Deviation of 1184.1755768907565	Mean of 515.7756666666667	Range of 4960.8	Minimum of 1.2	Maximum of 4962.0SigniNH2 Has: 	Standard Deviation of 19.359713538829944	Mean of 15.538166666666665	Range of 70.0	Minimum of 0.0	Maximum of 70.0NpH2 Has: 	Standard Deviation of 174.95321542915408	Mean of 82.98100000000001	Range of 871.0	Minimum of 0.0	Maximum of 871.0NpH2/Nbg Has: 	Standard Deviation of 24.28155154467413	Mean of 16.967499999999998	Range of 114.0	Minimum of 0.0	Maximum of 114.0NconvH2 Has: 	Standard Deviation of 21.242648843514996	Mean of 16.279666666666667	Range of 74.0	Minimum of 0.0	Maximum of 74.0NbgH2 Has: 	Standard Deviation of 32.49629154226679	Mean of 24.164	Range of 113.0	Minimum of 0.0	Maximum of 113.0FWHMaNH2 Has: 	Standard Deviation of 21.43746154839441	Mean of 13.954166666666667	Range of 97.0	Minimum of 0.0	Maximum of 97.0FWHMbNH2 Has: 	Standard Deviation of 25.9547814108178	Mean of 15.4955	Range of 142.0	Minimum of 0.0	Maximum of 142.0PANH2 Has: 	Standard Deviation of 36.533212245465386	Mean of 22.432833333333335	Range of 167.0	Minimum of 0.0	Maximum of 167.0NSED Has: 	Standard Deviation of 22.5229032221771	Mean of 16.332666666666665	Range of 99.0	Minimum of 0.0	Maximum of 99.0CSARflag Has: 	Standard Deviation of 22.485994042588676	Mean of 14.7255	Range of 107.0	Minimum of 0.0	Maximum of 107.0CUTEXflag Has: 	Standard Deviation of 39.146148392913446	Mean of 31.554	Range of 146.0	Minimum of 0.0	Maximum of 146.0